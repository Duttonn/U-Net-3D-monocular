# U-Net-3D-monocular

Projet d'estimation de profondeur monoculaire utilisant un U-Net 3D avec backbone ResNet18 et convolutions (2+1)D, entraÃ®nÃ© sur le dataset TartanAir V2.

**ğŸ¯ Architecture optimisÃ©e : UNet3D_Lite avec 14.1M paramÃ¨tres**

## âœ¨ FonctionnalitÃ©s

- **ğŸ—ï¸ ModÃ¨le UNet3D_Lite** : ResNet18 + (2+1)D convolutions (14.1M params)
- **ğŸ“Š Multi-tÃ¢ches** : Estimation de profondeur + segmentation sÃ©mantique
- **âš¡ OptimisÃ© Apple Silicon** : Support MPS (M1/M2/M3)
- **ğŸ“ˆ MÃ©triques complÃ¨tes** : AbsRel, RMSE, Î´<1.25
- **ğŸ¨ Visualisation intÃ©grÃ©e** : TensorBoard + script de visualisation rapide
- **ğŸ”§ Installation automatisÃ©e** : Script setup.sh clÃ© en main

## ğŸš€ Installation rapide

```bash
git clone <repo_url>
cd U-Net-3D-monocular/unet3d_mono
chmod +x setup.sh
./setup.sh
```

Le script d'installation automatique :
- âœ… CrÃ©e l'environnement virtuel
- âœ… Installe PyTorch avec support MPS
- âœ… GÃ©nÃ¨re 400 frames de donnÃ©es synthÃ©tiques
- âœ… Teste le modÃ¨le UNet3D_Lite
- âœ… PrÃ©pare l'environnement complet

## ğŸ“ Architecture du projet

```
unet3d_mono/
â”œâ”€â”€ setup.sh                     # ğŸ”§ Installation automatique
â”œâ”€â”€ run.sh                       # ğŸš€ Script de lancement rapide
â”œâ”€â”€ train.py                     # ğŸ¯ EntraÃ®nement principal
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ indoor_lowcam.yaml      # âš™ï¸ Configuration multi-env/camÃ©ras
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ unet3d_lite.py      # ğŸ—ï¸ UNet3D_Lite (14.1M params)
â”‚   â””â”€â”€ datasets/
â”‚       â”œâ”€â”€ simple_tartanair.py  # ğŸ“Š Dataset TartanAir V2
â”‚       â””â”€â”€ depth_utils.py       # ğŸ› ï¸ Utilitaires profondeur
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ download_expanded_synthetic.py  # ğŸ“¦ GÃ©nÃ©ration donnÃ©es Ã©tendues
â”‚   â””â”€â”€ quick_vis.py             # ğŸ‘ï¸ Visualisation rapide
â”œâ”€â”€ data/tartanair-v2/          # ğŸ’¾ DonnÃ©es (400 frames synthÃ©tiques)
â”œâ”€â”€ runs/                       # ğŸ“ˆ Logs TensorBoard
â””â”€â”€ ckpts/                      # ğŸ’¾ Checkpoints modÃ¨le
```

## ğŸ¯ Utilisation

### 1ï¸âƒ£ EntraÃ®nement rapide

```bash
source .venv/bin/activate
./run.sh
```

**RÃ©sultats attendus** (5 Ã©poques) :
```
Using device: mps
Params: 14.1M
Epoch 1/5: Loss: 2.24 â†’ AbsRel: 0.807
Epoch 5/5: Loss: 0.12 â†’ AbsRel: 0.029 (-96% amÃ©lioration!)
Final metrics: Î´<1.25: 3.816 (excellente prÃ©cision)
```

### 2ï¸âƒ£ Visualisation rapide

```bash
python scripts/quick_vis.py
```

GÃ©nÃ¨re automatiquement :
- `quick_rgb_center.png` - Image RGB centrale
- `quick_depth_pred.png` - PrÃ©diction de profondeur  
- `quick_depth_gt.png` - Ground truth profondeur
- `quick_seg_pred.png` - PrÃ©diction segmentation
- `quick_seg_gt.png` - Ground truth segmentation

### 3ï¸âƒ£ Monitoring TensorBoard

```bash
source .venv/bin/activate
tensorboard --logdir runs/indoor_lowcam
# Ouvrir http://localhost:6006
```

## âš™ï¸ Configuration avancÃ©e

### Multi-environnements et camÃ©ras

Modifier `configs/indoor_lowcam.yaml` :

```yaml
# Dataset Ã©largi (400 frames)
envs: ["ArchVizTinyHouseDay", "ArchVizTinyHouseNight"]
difficulties: ["easy", "hard"] 
cameras: ["lcam_bottom", "lcam_front"]  # lcam_bottom = vue basse

# EntraÃ®nement
epochs: 10          # Plus d'Ã©poques pour convergence
bs: 4              # Batch size (ajuster selon mÃ©moire)
lr: 2.5e-4         # Learning rate optimisÃ©
num_classes: 6     # Classes de segmentation
```

### GÃ©nÃ©rer plus de donnÃ©es

```bash
python scripts/download_expanded_synthetic.py \
  --frames_per_env 100 \
  --envs ArchVizTinyHouseDay ArchVizTinyHouseNight \
  --difficulties easy hard \
  --cameras lcam_bottom lcam_front
```

## ğŸ—ï¸ Architecture technique

### ModÃ¨le UNet3D_Lite

- **Backbone** : ResNet18 prÃ©entraÃ®nÃ© (avec fix SSL)
- **Fusion temporelle** : Convolutions (2+1)D pour T=5 frames
- **DÃ©codeur** : Upsampling progressif avec skip connections
- **Sorties duales** : Profondeur (1 canal) + Segmentation (6 classes)
- **Taille** : 14.1M paramÃ¨tres (optimal pour entraÃ®nement rapide)

### Pipeline de donnÃ©es

```python
Input: [B, T=5, 3, H=240, W=320]  # SÃ©quence de 5 images RGB
â”œâ”€â”€ Temporal adapter (3D conv)      # Fusion temporelle initiale
â”œâ”€â”€ ResNet18 encoder               # Extraction features spatiales
â”œâ”€â”€ (2+1)D decoder                # DÃ©codage avec fusion temporelle
â””â”€â”€ Dual heads                    # Profondeur + Segmentation
    â”œâ”€â”€ Depth: [B, 1, H, W]      # 0-10m profondeur
    â””â”€â”€ Seg: [B, 6, H, W]        # 6 classes sÃ©mantiques
```

### MÃ©triques de profondeur

- **AbsRel** : Erreur relative absolue (â†“ mieux)
- **RMSE** : Erreur quadratique moyenne (â†“ mieux)  
- **Î´<1.25** : % pixels avec ratio < 1.25 (â†‘ mieux)

## ğŸ”§ DÃ©veloppement

### Ajouter un nouveau modÃ¨le

1. CrÃ©er `src/models/mon_modele.py`
2. ImplÃ©menter avec l'interface :
```python
class MonModele(nn.Module):
    def forward(self, x):  # x: [B, T, 3, H, W]
        return depth_pred, seg_logits  # [B,1,H,W], [B,C,H,W]
```
3. Remplacer dans `train.py` :
```python
from src.models.mon_modele import MonModele
model = MonModele(num_classes=6)
```

### Optimisations performances

- **Apple Silicon** : DÃ©tection auto MPS/CUDA/CPU
- **MÃ©moire** : `channels_last` format pour efficacitÃ©
- **Threading** : `num_workers=0` (TartanAir gÃ¨re la parallÃ©lisation)

## ğŸ“Š RÃ©sultats

### Dataset synthÃ©tique Ã©largi (400 frames)

| MÃ©trique | Epoch 1 | Epoch 5 | AmÃ©lioration |
|----------|---------|---------|--------------|
| Loss     | 2.24    | 0.12    | -95%         |
| AbsRel   | 0.807   | 0.029   | -96%         |
| RMSE     | 2.227   | 0.140   | -94%         |
| Î´<1.25   | 0.500   | 3.816   | +660%        |

**ğŸ‰ Convergence excellente avec le modÃ¨le UNet3D_Lite !**

## ğŸ› ï¸ Troubleshooting

### Installation macOS

```bash
# Si problÃ¨me SSL (certificats)
/Applications/Python\ 3.x/Install\ Certificates.command

# Si PyTorch MPS non dÃ©tectÃ©
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
```

### MÃ©moire insuffisante

```yaml
# RÃ©duire dans configs/indoor_lowcam.yaml
bs: 2                # Batch size plus petit
size: [180, 240]     # RÃ©solution rÃ©duite
T: 3                 # Moins de frames temporelles
```

### Checkpoints corrompus

```bash
# Nettoyer et redÃ©marrer
rm -rf ckpts/indoor_lowcam/
rm -rf runs/indoor_lowcam/
./run.sh
```

## ğŸ“š DÃ©pendances principales

- **torch â‰¥2.0.0** - Framework ML avec support MPS
- **torchvision** - ModÃ¨les prÃ©entraÃ®nÃ©s (ResNet18)
- **opencv-python** - Traitement d'images et profondeur
- **tensorboard** - Monitoring d'entraÃ®nement
- **PyYAML** - Configuration YAML

## ğŸ† Prochaines Ã©tapes

1. **Vraies donnÃ©es TartanAir V2** - Remplacer donnÃ©es synthÃ©tiques
2. **Optimisations M3** - Ajouter `autocast(bfloat16)` pour vitesse
3. **Fonctions de perte avancÃ©es** - ImplÃ©menter dans `src/losses/`
4. **Dataset multi-env** - Utiliser tous les environnements simultanÃ©ment

## ğŸ“„ Licence

Ce projet est destinÃ© Ã  des fins Ã©ducatives. Le dataset TartanAir V2 a ses propres [conditions d'utilisation](http://theairlab.org/tartanair-dataset/).